import spacy
import pandas as pd
import re
from spacy.matcher import PhraseMatcher
from spacy.lang.fr.stop_words import STOP_WORDS

# Charger le mod√®le NLP
nlp = spacy.load("fr_core_news_lg")

# D√©finition des chemins des fichiers
df_clean_path = "./data/datasets/brut/df_final2.csv"
df_output_path = "./data/datasets/propre/df_clean2_nlp.csv"

# Charger df_clean
df_clean = pd.read_csv(df_clean_path, encoding="utf-8")

# V√©rifier la pr√©sence des colonnes n√©cessaires
if "Profile" not in df_clean.columns or "Description" not in df_clean.columns:
    raise ValueError("Les colonnes 'Profile' et 'Description' sont requises dans df_clean.")


# Fonction de nettoyage avanc√© du texte
def nettoyer_texte(texte):
    if pd.isna(texte):
        return ""
    texte = texte.lower()
    texte = re.sub(r"[^a-zA-Z√Ä-√ø0-9\s]", "", texte)  # Supprimer les caract√®res sp√©ciaux
    doc = nlp(texte)  # Transformer en objet spaCy
    tokens = [token.lemma_ for token in doc if token.text not in STOP_WORDS and not token.is_punct]
    return " ".join(tokens)


# Appliquer le nettoyage et la lemmatisation
df_clean["Profile"] = df_clean["Profile"].apply(nettoyer_texte)
df_clean["Description"] = df_clean["Description"].apply(nettoyer_texte)


# Fonction pour construire les matchers avec lemmatisation des termes
def construire_matcher(dictionnaire):
    matcher = PhraseMatcher(nlp.vocab, attr="LEMMA")  # Prend en compte les formes de base (lemmatisation)
    for categorie, termes in dictionnaire.items():
        patterns = [nlp(terme.lower()) for terme in termes]  # NLP sur chaque mot-cl√©
        matcher.add(categorie, patterns)
    return matcher


# Dictionnaires de mots-cl√©s (formes lemmatis√©es et variantes incluses)
competences_cles = {
    "SQL et bases de donn√©es": ["sql", "base de donn√©es", "requ√™te", "jointure", "index", "optimisation"],
    "Python": ["python", "pandas", "numpy", "programmation", "data analysis"],
    "Statistiques et probabilit√©s": ["statistique", "probabilit√©", "analyse statistique", "r√©gression"],
    "Nettoyage et transformation des donn√©es": ["etl", "data cleaning", "transformation", "pipelines"],
    "Visualisation et storytelling": ["visualisation", "dashboard", "tableau de bord", "reporting", "graphique"],
    "Cloud et Big Data": ["aws", "gcp", "azure", "hadoop", "big data"],
    "Automatisation et scripting": ["automatisation", "script", "workflow", "shell", "airflow"],
    "Gestion des API et Web Scraping": ["api", "web scraping", "json", "rest"],
    "Mod√©lisation et machine learning": ["machine learning", "mod√®le pr√©dictif", "classification", "r√©gression"],
    "D√©ploiement et industrialisation": ["d√©ploiement", "ci/cd", "docker", "fastapi", "flask", "mlops"]
}

soft_skills = {
    "Esprit analytique": ["analyse", "mod√©lisation", "logique"],
    "Curiosit√© et apprentissage continu": ["curiosit√©", "formation", "autodidacte"],
    "Communication et vulgarisation": ["communication", "explication", "pr√©sentation"],
    "R√©solution de probl√®mes": ["probl√®me", "solution", "optimisation"],
    "Collaboration et travail en √©quipe": ["√©quipe", "collaboration", "partage"],
    "Autonomie et prise d‚Äôinitiative": ["autonomie", "proactivit√©"],
    "Adaptabilit√© et agilit√©": ["adaptabilit√©", "agile", "scrum"],
    "Gestion du temps et organisation": ["gestion du temps", "priorisation"],
    "Esprit critique": ["sens critique", "remise en question"],
    "Sens du d√©tail et rigueur": ["rigueur", "pr√©cision"]
}

outils = {
    "SQL": ["sql", "postgresql", "mysql", "bigquery"],
    "Python": ["python", "pandas", "numpy", "scikit-learn"],
    "Excel & Google Sheets": ["excel", "tableau crois√© dynamique", "vba"],
    "Power BI / Tableau / Looker": ["power bi", "tableau", "looker"],
    "Jupyter Notebook & VS Code": ["jupyter", "vs code"],
    "Apache Airflow & dbt": ["airflow", "dbt"],
    "Git & GitHub/GitLab": ["git", "github", "gitlab"],
    "Docker & Kubernetes": ["docker", "kubernetes"],
    "Cloud Computing": ["aws", "gcp", "azure"],
    "Google Colab": ["colab", "google colab"]
}

# Initialiser les matchers avec les dictionnaires optimis√©s
matcher_competences = construire_matcher(competences_cles)
matcher_soft_skills = construire_matcher(soft_skills)
matcher_outils = construire_matcher(outils)


# üîπ Fonction de d√©tection des mots-cl√©s avec NLP et lemmatisation
def detecter_mots_cles(texte, matcher):
    doc = nlp(texte)
    mots_cles = set()
    for match_id, start, end in matcher(doc):
        mots_cles.add(nlp.vocab.strings[match_id])  # R√©cup√©rer la cat√©gorie correspondante
    return list(mots_cles)


# Appliquer la d√©tection NLP aux colonnes
df_clean["Competences_Cl√©s"] = df_clean["Profile"].apply(lambda x: detecter_mots_cles(x, matcher_competences)) + \
                               df_clean["Description"].apply(lambda x: detecter_mots_cles(x, matcher_competences))

df_clean["Soft_Skills"] = df_clean["Profile"].apply(lambda x: detecter_mots_cles(x, matcher_soft_skills)) + \
                          df_clean["Description"].apply(lambda x: detecter_mots_cles(x, matcher_soft_skills))

df_clean["Outils"] = df_clean["Profile"].apply(lambda x: detecter_mots_cles(x, matcher_outils)) + \
                     df_clean["Description"].apply(lambda x: detecter_mots_cles(x, matcher_outils))

# Supprimer les doublons
df_clean["Competences_Cl√©s"] = df_clean["Competences_Cl√©s"].apply(lambda x: list(set(x)))
df_clean["Soft_Skills"] = df_clean["Soft_Skills"].apply(lambda x: list(set(x)))
df_clean["Outils"] = df_clean["Outils"].apply(lambda x: list(set(x)))

# Sauvegarde du fichier
df_clean.to_csv(df_output_path, index=False, encoding="utf-8")
print(f"Fichier enregistr√© : {df_output_path}")
